{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solomontessema/Generative-AI-with-Python/blob/main/notebooks/Vector_Databases_with_Pinecone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone"
      ],
      "metadata": {
        "id": "olnA_XDNblpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vec2text"
      ],
      "metadata": {
        "id": "DJsZA0xyislo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2LsoQruewpq"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pinecone\n",
        "import openai\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Create Pinecone client\n",
        "pc = Pinecone(api_key=api_key)\n",
        "\n",
        "# Create index if it doesn't exist\n",
        "if \"my-index\" not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=\"my-index\",\n",
        "        dimension=1536,  # Match your embedding model output\n",
        "        metric=\"cosine\",  # Or 'euclidean', 'dotproduct'\n",
        "        spec=ServerlessSpec(\n",
        "            cloud=\"aws\",  # Or 'gcp'\n",
        "            region=\"us-east-1\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Connect to index\n",
        "index = pc.Index(\"my-index\")\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "# Embed text using OpenAI\n",
        "def embed(text):\n",
        "    response = client.embeddings.create(\n",
        "        input=[text],\n",
        "        model=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# Upsert vector\n",
        "index.upsert([\n",
        "    (\"concept1\", embed(\"The theory of relativity and its implications\"), {\"topic\": \"physics\"}),\n",
        "    (\"concept2\", embed(\"The role of empathy in leadership\"), {\"topic\": \"psychology\"})\n",
        "])\n",
        "\n",
        "# Query\n",
        "query_vector = embed(\"My cat is so cute\")\n",
        "results = index.query(vector=query_vector, top_k=5, include_metadata=True)\n",
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector = embed(\"The theory of relativity\")\n",
        "results = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=5,\n",
        "    include_metadata=True,\n",
        "    filter={\"topic\": \"physics\"}  # Only return vectors tagged with topic 'physics'\n",
        ")\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "W8tpVR3WiXwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating an existing vector by ID\n",
        "index.upsert([\n",
        "    (\"concept1\", embed(\"Updated theory of relativity and its modern implications\"), {\"topic\": \"physics\"})\n",
        "])\n",
        "\n",
        "# Deleting a vector by ID\n",
        "index.delete(ids=[\"concept2\"])\n",
        "\n",
        "all_vectors = index.query(\n",
        "    vector=embed(\"The theory of relativity\"),  # Use a relevant query string here\n",
        "    top_k=10,\n",
        "    include_metadata=True\n",
        ")\n",
        "print(all_vectors)\n"
      ],
      "metadata": {
        "id": "9K1AT7il2Mi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Upserts and Queries Explore how to efficiently upsert and query large batches of vectors, which is essential for scaling your application.\n",
        "\n",
        "Handling Different Similarity Metrics Experiment with different similarity metrics (cosine, euclidean, dotproduct) and understand how they affect search results.\n",
        "\n",
        "Integrating Pinecone with Applications Build simple applications or APIs that use Pinecone for semantic search or recommendations, combining it with OpenAI for embedding generation.\n",
        "\n",
        "Performance and Cost Optimization Learn best practices for index dimension sizing, vector pruning, and query tuning to optimize speed and cost."
      ],
      "metadata": {
        "id": "aFLa6uxM3asg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch upsert example\n",
        "texts = [\n",
        "    \"Quantum mechanics and its applications\",\n",
        "    \"The psychology of motivation\",\n",
        "    \"Advances in renewable energy technology\"\n",
        "]\n",
        "\n",
        "vectors_to_upsert = []\n",
        "for i, text in enumerate(texts, start=3):\n",
        "    vectors_to_upsert.append((f\"concept{i}\", embed(text), {\"topic\": \"mixed\"}))\n",
        "\n",
        "index.upsert(vectors_to_upsert)\n",
        "\n",
        "# Query after batch upsert\n",
        "query_vector = embed(\"renewable energy\")\n",
        "results = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "-9SbD74s3YM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling Different Similarity Metrics Experiment with different similarity metrics (cosine, euclidean, dotproduct) and understand how they affect search results.\n"
      ],
      "metadata": {
        "id": "jwqBzB169zj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create indexes with different metrics\n",
        "for metric in [\"cosine\", \"euclidean\", \"dotproduct\"]:\n",
        "    if metric not in pc.list_indexes().names():\n",
        "        pc.create_index(\n",
        "            name=f\"my-index-{metric}\",\n",
        "            dimension=1536,\n",
        "            metric=metric,\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "        )\n",
        "\n",
        "# Connect to a specific index\n",
        "index_cosine = pc.Index(\"my-index-cosine\")\n",
        "index_euclidean = pc.Index(\"my-index-euclidean\")\n",
        "index_dotproduct = pc.Index(\"my-index-dotproduct\")\n",
        "\n",
        "# Upsert the same vectors to each index\n",
        "vectors = [\n",
        "    (\"concept1\", embed(\"The theory of relativity and its implications\"), {\"topic\": \"physics\"}),\n",
        "    (\"concept2\", embed(\"The role of empathy in leadership\"), {\"topic\": \"psychology\"})\n",
        "]\n",
        "for idx in [index_cosine, index_euclidean, index_dotproduct]:\n",
        "    idx.upsert(vectors)\n"
      ],
      "metadata": {
        "id": "j7LVI9Gu9teg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to a specific index\n",
        "index_cosine = pc.Index(\"my-index-cosine\")\n",
        "index_euclidean = pc.Index(\"my-index-euclidean\")\n",
        "index_dotproduct = pc.Index(\"my-index-dotproduct\")\n",
        "\n",
        "# Upsert the same vectors to each index\n",
        "vectors = [\n",
        "    (\"concept1\", embed(\"The theory of relativity and its implications\"), {\"topic\": \"physics\"}),\n",
        "    (\"concept2\", embed(\"The role of empathy in leadership\"), {\"topic\": \"psychology\"})\n",
        "]\n",
        "for idx in [index_cosine, index_euclidean, index_dotproduct]:\n",
        "    idx.upsert(vectors)"
      ],
      "metadata": {
        "id": "lbZMMDecA0Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Query each index with the same query vector\n",
        "query_vector = embed(\"The theory of relativity\")\n",
        "results_cosine = index_cosine.query(vector=query_vector, top_k=5, include_metadata=True)\n",
        "results_euclidean = index_euclidean.query(vector=query_vector, top_k=5, include_metadata=True)\n",
        "results_dotproduct = index_dotproduct.query(vector=query_vector, top_k=5, include_metadata=True)\n",
        "\n",
        "print(\"Cosine similarity results:\", results_cosine)\n",
        "print(\"Euclidean distance results:\", results_euclidean)\n",
        "print(\"Dot product results:\", results_dotproduct)"
      ],
      "metadata": {
        "id": "cVta1q0xA-JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrating Pinecone with Applications Build simple applications or APIs that use Pinecone for semantic search or recommendations, combining it with OpenAI for embedding generation.\n",
        "\n"
      ],
      "metadata": {
        "id": "AcvYDJFO96EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import pinecone\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = pinecone.Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "index_name = \"my-index\"\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str\n",
        "    top_k: int = 5\n",
        "\n",
        "# Helper function to embed text\n",
        "async def embed_text(text: str):\n",
        "    response = client.embeddings.create(\n",
        "        input=[text],\n",
        "        model=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "@app.post(\"/search\")\n",
        "async def search(request: QueryRequest):\n",
        "    if not request.query:\n",
        "        raise HTTPException(status_code=400, detail=\"Query text is required\")\n",
        "\n",
        "    query_vector = await embed_text(request.query)\n",
        "    results = index.query(\n",
        "        vector=query_vector,\n",
        "        top_k=request.top_k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    return {\"matches\": results.matches}\n"
      ],
      "metadata": {
        "id": "C8IDzpzL99bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance and Cost Optimization Learn best practices for index dimension sizing, vector pruning, and query tuning to optimize speed and cost."
      ],
      "metadata": {
        "id": "gGo_gEMW991R"
      }
    }
  ]
}