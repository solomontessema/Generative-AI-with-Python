
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solomontessema/Generative-AI-with-Python/blob/main/notebooks/new_Vector_Database_with_Pinecone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]    
     },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview"
      },
      "source": [
        "## What you'll do\n",
        "\n",
        "1. Install and configure **Pinecone** and **OpenAI**.\n",
        "2. Review how similarity metrics work and when to use each.\n",
        "3. Create **three Pinecone indexes** (cosine / euclidean / dotproduct).\n",
        "4. Upsert the **same vectors** into each index.\n",
        "5. Run the **same query** against each index and compare results.\n",
        "6. Try **metadata filtering**, **updates**, and **deletes**.\n",
        "7. (Optional) See a **FastAPI** example for a semantic search API.\n",
        "\n",
        "> ðŸ§ª This notebook is designed for experimentationâ€”feel free to tweak texts, add more vectors, and observe how rankings change across metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "!pip -q install pinecone-client openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keys"
      },
      "source": [
        "## Configure API keys (Pinecone + OpenAI)\n",
        "\n",
        "- You can paste your keys when prompted, or set them in a `.env` file.\n",
        "- **Never** commit secrets to version control.\n",
        "\n",
        "**Required env vars:**\n",
        "- `PINECONE_API_KEY`\n",
        "- `OPENAI_API_KEY`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "if not os.getenv(\"PINECONE_API_KEY\"):\n",
        "    os.environ[\"PINECONE_API_KEY\"] = getpass(\"Enter PINECONE_API_KEY: \")\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OPENAI_API_KEY: \")\n",
        "\n",
        "print(\"Pinecone key set:\", bool(os.getenv(\"PINECONE_API_KEY\")))\n",
        "print(\"OpenAI key set:  \", bool(os.getenv(\"OPENAI_API_KEY\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "metrics_explainer"
      },
      "source": [
        "## Handling Different Similarity Metrics\n",
        "\n",
        "**Cosine similarity**\n",
        "- Measures the angle between vectors (direction-only).\n",
        "- Popular for text embeddings; magnitude differences are downplayed.\n",
        "\n",
        "**Euclidean distance**\n",
        "- Straight-line distance between points in space.\n",
        "- Sensitive to vector magnitude and scale.\n",
        "\n",
        "**Dot product**\n",
        "- Sum of element-wise products.\n",
        "- Can emphasize vectors with larger magnitudes (length matters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_clients"
      },
      "source": [
        "## Initialize clients & helpers\n",
        "\n",
        "We will:\n",
        "1. Initialize the Pinecone client (serverless spec).\n",
        "2. Initialize the OpenAI client.\n",
        "3. Define a reusable `embed()` function using **`text-embedding-3-small`** (1536-dim)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clients_code"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from openai import OpenAI\n",
        "\n",
        "PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n",
        "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "EMBED_MODEL = \"text-embedding-3-small\"  # 1536-dim\n",
        "EMBED_DIM = 1536\n",
        "\n",
        "def embed(text: str):\n",
        "    \"\"\"Return a single 1536-dim embedding vector for the given text.\"\"\"\n",
        "    resp = client.embeddings.create(model=EMBED_MODEL, input=[text])\n",
        "    return resp.data[0].embedding\n",
        "\n",
        "print(\"Clients ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_indexes_md"
      },
      "source": [
        "## Create three indexes with different metrics\n",
        "\n",
        "We'll create:\n",
        "- `my-index-cosine` (cosine)\n",
        "- `my-index-euclidean` (euclidean)\n",
        "- `my-index-dotproduct` (dotproduct)\n",
        "\n",
        "âš ï¸ Index creation can take a few seconds. If the index exists, we reuse it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_indexes_code"
      },
      "outputs": [],
      "source": [
        "CLOUD = \"aws\"\n",
        "REGION = \"us-east-1\"\n",
        "\n",
        "index_names = {\n",
        "    \"cosine\": \"my-index-cosine\",\n",
        "    \"euclidean\": \"my-index-euclidean\",\n",
        "    \"dotproduct\": \"my-index-dotproduct\"\n",
        "}\n",
        "\n",
        "existing = set(pc.list_indexes().names())\n",
        "\n",
        "for metric, name in index_names.items():\n",
        "    if name not in existing:\n",
        "        print(f\"Creating {name} with metric={metric}...\")\n",
        "        pc.create_index(\n",
        "            name=name,\n",
        "            dimension=EMBED_DIM,\n",
        "            metric=metric,\n",
        "            spec=ServerlessSpec(cloud=CLOUD, region=REGION)\n",
        "        )\n",
        "    else:\n",
        "        print(f\"{name} already exists; skipping creation.\")\n",
        "\n",
        "index_cosine = pc.Index(index_names[\"cosine\"]) \n",
        "index_euclidean = pc.Index(index_names[\"euclidean\"]) \n",
        "index_dotproduct = pc.Index(index_names[\"dotproduct\"]) \n",
        "\n",
        "print(\"Indexes are ready for use.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upsert_md"
      },
      "source": [
        "## Upsert the same vectors into each index\n",
        "\n",
        "We'll use two short texts with distinct topics and add a `topic` metadata field to each. \n",
        "\n",
        "This makes it easy to demonstrate **metadata filtering** later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upsert_code"
      },
      "outputs": [],
      "source": [
        "texts = {\n",
        "    \"concept1\": {\n",
        "        \"text\": \"The theory of relativity and its implications\",\n",
        "        \"metadata\": {\"topic\": \"physics\"}\n",
        "    },\n",
        "    \"concept2\": {\n",
        "        \"text\": \"The role of empathy in leadership\",\n",
        "        \"metadata\": {\"topic\": \"psychology\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "def upsert_to_index(index, items):\n",
        "    vectors = []\n",
        "    for _id, payload in items.items():\n",
        "        vec = embed(payload[\"text\"])  # 1536-dim\n",
        "        vectors.append({\"id\": _id, \"values\": vec, \"metadata\": payload[\"metadata\"]})\n",
        "    index.upsert(vectors=vectors)\n",
        "\n",
        "for idx in (index_cosine, index_euclidean, index_dotproduct):\n",
        "    upsert_to_index(idx, texts)\n",
        "\n",
        "print(\"Upserted vectors into all three indexes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "query_md"
      },
      "source": [
        "## Query each index with the same query vector\n",
        "\n",
        "We'll query for **â€œThe theory of relativityâ€** and compare the top-k results from each index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "query_code"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "query_text = \"The theory of relativity\"\n",
        "query_vec = embed(query_text)\n",
        "\n",
        "def run_query(index, vector, top_k=5, with_metadata=True):\n",
        "    return index.query(\n",
        "        vector=vector,\n",
        "        top_k=top_k,\n",
        "        include_metadata=with_metadata\n",
        "    )\n",
        "\n",
        "res_cosine = run_query(index_cosine, query_vec)\n",
        "res_euclid = run_query(index_euclidean, query_vec)\n",
        "res_dot   = run_query(index_dotproduct, query_vec)\n",
        "\n",
        "print(\"\\n=== Cosine similarity results ===\")\n",
        "pprint([{\"id\": m.id, \"score\": m.score, \"metadata\": m.metadata} for m in res_cosine.matches])\n",
        "\n",
        "print(\"\\n=== Euclidean distance results ===\")\n",
        "pprint([{\"id\": m.id, \"score\": m.score, \"metadata\": m.metadata} for m in res_euclid.matches])\n",
        "\n",
        "print(\"\\n=== Dot product results ===\")\n",
        "pprint([{\"id\": m.id, \"score\": m.score, \"metadata\": m.metadata} for m in res_dot.matches])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interpretation_md"
      },
      "source": [
        "## Understanding the impact\n",
        "\n",
        "- **Cosine similarity** focuses on vector *direction* â†’ good for text where magnitude differences arenâ€™t meaningful.\n",
        "- **Euclidean distance** uses absolute distance â†’ can be influenced by magnitude/scale.\n",
        "- **Dot product** increases with both alignment and magnitude â†’ longer vectors can score higher.\n",
        "\n",
        "> âœ¨ Tip: For modern text embeddings, cosine is a great default. If your pipeline preserves meaningful magnitudes (e.g., TF-IDF-like scaling) or you need geometric distance, experiment with dot product or euclidean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "filter_md"
      },
      "source": [
        "## Metadata filtering example\n",
        "\n",
        "You can filter matches by metadataâ€”e.g., return only items where `topic == \"physics\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "filter_code"
      },
      "outputs": [],
      "source": [
        "res_filtered = index_cosine.query(\n",
        "    vector=query_vec,\n",
        "    top_k=5,\n",
        "    include_metadata=True,\n",
        "    filter={\"topic\": {\"$eq\": \"physics\"}}\n",
        ")\n",
        "print(\"Filtered (topic=physics) on cosine index:\")\n",
        "pprint([{\"id\": m.id, \"score\": m.score, \"metadata\": m.metadata} for m in res_filtered.matches])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "update_delete_md"
      },
      "source": [
        "## Updating and deleting vectors\n",
        "\n",
        "- **Update** by upserting with the same ID.\n",
        "- **Delete** by ID.\n",
        "- Re-query to verify the change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "update_delete_code"
      },
      "outputs": [],
      "source": [
        "# Update concept1 text\n",
        "updated_text = \"Updated theory of relativity and its modern implications\"\n",
        "index_cosine.upsert(\n",
        "    vectors=[{\n",
        "        \"id\": \"concept1\",\n",
        "        \"values\": embed(updated_text),\n",
        "        \"metadata\": {\"topic\": \"physics\", \"version\": \"updated\"}\n",
        "    }]\n",
        ")\n",
        "\n",
        "# Delete concept2\n",
        "index_cosine.delete(ids=[\"concept2\"]) \n",
        "\n",
        "# Re-query to see new state (broad query)\n",
        "broad_vec = embed(\"theory\")\n",
        "res_after = index_cosine.query(vector=broad_vec, top_k=10, include_metadata=True)\n",
        "pprint([{\"id\": m.id, \"score\": m.score, \"metadata\": m.metadata} for m in res_after.matches])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fastapi_md"
      },
      "source": [
        "## (Optional) Integrating with an application (FastAPI example)\n",
        "\n",
        "Below is a minimal FastAPI endpoint that:\n",
        "1. Accepts a query string\n",
        "2. Embeds with OpenAI\n",
        "3. Queries a Pinecone index\n",
        "4. Returns top-k matches\n",
        "\n",
        "This is **for reference**â€”running FastAPI in Colab requires extra steps (e.g., `uvicorn` + tunnels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fastapi_code"
      },
      "outputs": [],
      "source": [
        "FASTAPI_SNIPPET = r'''\\nfrom fastapi import FastAPI, HTTPException\\nfrom pydantic import BaseModel\\nfrom pinecone import Pinecone\\nfrom openai import OpenAI\\nimport os\\n\\napp = FastAPI()\\n\\npc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\\nindex = pc.Index(\"my-index-cosine\")  # choose your index\\nclient = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\\n\\nEMBED_MODEL = \"text-embedding-3-small\"\\n\\nclass QueryRequest(BaseModel):\\n    query: str\\n    top_k: int = 5\\n\\ndef embed(text: str):\\n    resp = client.embeddings.create(model=EMBED_MODEL, input=[text])\\n    return resp.data[0].embedding\\n\\n@app.post(\"/search\")\\ndef search(req: QueryRequest):\\n    if not req.query:\\n        raise HTTPException(status_code=400, detail=\"query is required\")\\n    qv = embed(req.query)\\n    results = index.query(vector=qv, top_k=req.top_k, include_metadata=True)\\n    return {\"matches\": [\\n        {\"id\": m.id, \"score\": m.score, \"metadata\": m.metadata} for m in results.matches\\n    ]}\\n'''\\nprint(FASTAPI_SNIPPET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup_md"
      },
      "source": [
        "## (Optional) Cleanup\n",
        "\n",
        "Delete the demo indexes to avoid clutter and charges. Set `DO_DELETE = True` to proceed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup_code"
      },
      "outputs": [],
      "source": [
        "DO_DELETE = False  # set to True to delete the demo indexes\n",
        "\n",
        "if DO_DELETE:\n",
        "    for name in index_names.values():\n",
        "        print(\"Deleting:\", name)\n",
        "        pc.delete_index(name)\n",
        "    print(\"Deleted demo indexes.\")\n",
        "else:\n",
        "    print(\"Skipping delete. Set DO_DELETE=True to remove demo indexes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "- Add more documents and experiment with **longer texts** or **different domains**.\n",
        "- Try **hybrid search** (lexical + vector) by combining scores externally.\n",
        "- Attach **payloads** such as URLs, timestamps, authors in metadata and filter on them.\n",
        "- Benchmark metrics for your task (e.g., retrieval for QA vs. recommendations)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "colab": {
      "name": "Vector Databases with Pinecone - Similarity Metrics.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
