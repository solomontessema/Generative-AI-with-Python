{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solomontessema/Generative-AI-with-Python/blob/main/notebooks/Introduction_to_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeDwzwQ0n3fH"
      },
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <td><img src=\"https://ionnova.com/img/ionnova_logo_name_2.png\" width=\"120px\"></td>\n",
        "    <td><h1>Day 12: RAG (Retrieval-Augmented Generation)</h1></td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gpufwQLJ5oc",
        "outputId": "fe68b746-bbae-4aef-f0f3-472e89cd515f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to IonnovaBot! Type 'exit' to quit.\n",
            "\n",
            "You: who are ionnova?\n",
            "IonnovaBot: IONNOVA is a technology company based in Frisco, Texas. They specialize in software engineering, AI automation, and data analytics. They design and develop custom web, mobile, and enterprise applications for businesses, focusing on automation, scalability, and performance optimization. Their work also involves building intelligent systems using machine learning, deep learning, and natural language processing for predictive analytics, decision-making, and business process automation. In addition, they offer advanced data analytics, AutoCAD customization, professional CAD drafting, direct placement services for IT professionals, and training in software development, data analytics, and AI. Their mission is to empower businesses through innovative software, intelligent automation, and data-driven strategies.\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "API_KEY = os.getenv(\"GPT_API_KEY\")\n",
        "client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "# Load knowledge base (simple text chunks for demo)\n",
        "def load_knowledge_base():\n",
        "    with open(\"knowledge_base.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        chunks = f.read().split(\"\\n\\n\")  # Assume chunks are separated by double newlines\n",
        "        #print(chunks)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chat_with_rag():\n",
        "    print(\"Welcome to IonnovaBot! Type 'exit' to quit.\\n\")\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant that uses external knowledge to answer questions.\"}]\n",
        "    kb_chunks = load_knowledge_base()\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"IonnovaBot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Retrieve context\n",
        "\n",
        "        context = \"\\n---\\n\".join(kb_chunks)\n",
        "\n",
        "        # Inject context into prompt\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {user_input}\"})\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        reply = response.choices[0].message.content\n",
        "        print(f\"IonnovaBot: {reply}\")\n",
        "        messages.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "chat_with_rag()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"GPT_API_KEY\")\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index_name = \"ionnova-rag\"\n",
        "if index_name not in pc.list_indexes():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Load and embed knowledge base\n",
        "def load_and_embed_kb():\n",
        "    with open(\"knowledge_base.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        chunks = f.read().split(\"\\n\\n\")\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        embedding = client.embeddings.create(\n",
        "            model=\"text-embedding-ada-002\",\n",
        "            input=chunk\n",
        "        ).data[0].embedding\n",
        "        index.upsert([(f\"chunk-{i}\", embedding, {\"text\": chunk})])\n",
        "\n",
        "# Retrieve top-k relevant chunks\n",
        "def retrieve_context(query, top_k=5):\n",
        "    query_embedding = client.embeddings.create(\n",
        "        model=\"text-embedding-ada-002\",\n",
        "        input=query\n",
        "    ).data[0].embedding\n",
        "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
        "    return [match[\"metadata\"][\"text\"] for match in results[\"matches\"]]\n",
        "\n",
        "# Chat loop\n",
        "def chat_with_rag():\n",
        "    print(\"Welcome to IonnovaBot! Type 'exit' to quit.\\n\")\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant that uses external knowledge to answer questions.\"}]\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"IonnovaBot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        context_chunks = retrieve_context(user_input)\n",
        "        context = \"\\n---\\n\".join(context_chunks)\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {user_input}\"})\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        reply = response.choices[0].message.content\n",
        "        print(f\"IonnovaBot: {reply}\")\n",
        "        messages.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "# Run once to populate Pinecone\n",
        "# load_and_embed_kb()\n",
        "\n",
        "chat_with_rag()\n"
      ],
      "metadata": {
        "id": "8JCNsWdJFuvm",
        "outputId": "abe61bf4-fdc2-4025-91b6-3ed0a7c0a04b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to IonnovaBot! Type 'exit' to quit.\n",
            "\n",
            "You: who are ionnova?\n",
            "IonnovaBot: I'm sorry, but I couldn't find any specific or detailed information about \"ionnova.\" It's possible that there might be a spelling mistake or the entity isn't well-known or doesn't exist. If you have additional details or context, I'd be happy to try and provide a better answer.\n",
            "You: What is IONNOVA?\n",
            "IonnovaBot: I'm sorry, but I still couldn't find any specific or detailed information about \"IONNOVA.\" It's possible that there might be a spelling mistake or the entity isn't widely recognized or doesn't exist. Please provide more details or context and I'll do my best to provide a helpful answer.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1165373106.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# load_and_embed_kb()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mchat_with_rag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1165373106.py\u001b[0m in \u001b[0;36mchat_with_rag\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"IonnovaBot: Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}