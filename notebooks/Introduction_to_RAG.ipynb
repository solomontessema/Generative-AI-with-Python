{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solomontessema/Generative-AI-with-Python/blob/main/notebooks/Introduction_to_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeDwzwQ0n3fH"
      },
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <td><img src=\"https://ionnova.com/img/ionnova_logo_name_2.png\" width=\"120px\"></td>\n",
        "    <td><h1>Day 12: RAG (Retrieval-Augmented Generation)</h1></td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔍 What is RAG?\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a technique that enhances language model responses by injecting external context—often referred to as a **knowledge base**—into the prompt. This allows the model to generate more accurate, grounded, and domain-specific answers.\n",
        "\n",
        "In this section, we demonstrate a simple RAG setup using a plain text file (`knowledge_base.txt`) as the knowledge source. When a user submits a question, the system appends it to the contents of the file and sends the combined context to a language model (LLM) for inference.\n",
        "\n",
        "### 🧪 Workflow Overview:\n",
        "- Load knowledge base from `knowledge_base.txt`\n",
        "- Append user query to the knowledge base\n",
        "- Pass the combined context to GPT\n",
        "- Return a contextualized response\n",
        "\n",
        "> This foundational approach sets the stage for more advanced RAG systems using semantic search and vector databases.\n"
      ],
      "metadata": {
        "id": "5_SaX8LfN7k-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gpufwQLJ5oc"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "API_KEY = os.getenv(\"GPT_API_KEY\")\n",
        "client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "# Load knowledge base (simple text chunks for demo)\n",
        "def load_knowledge_base():\n",
        "    with open(\"knowledge_base.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        chunks = f.read().split(\"\\n\\n\")  # Assume chunks are separated by double newlines\n",
        "        #print(chunks)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chat_with_rag():\n",
        "    print(\"Welcome to IonnovaBot! Type 'exit' to quit.\\n\")\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant that uses external knowledge to answer questions.\"}]\n",
        "    kb_chunks = load_knowledge_base()\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"IonnovaBot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Retrieve context\n",
        "\n",
        "        context = \"\\n---\\n\".join(kb_chunks)\n",
        "\n",
        "        # Inject context into prompt\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {user_input}\"})\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        reply = response.choices[0].message.content\n",
        "        print(f\"IonnovaBot: {reply}\")\n",
        "        messages.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "chat_with_rag()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔁 Retrieval-Augmented Generation with Pinecone\n",
        "\n",
        "This section builds on the previous text-based RAG demo by introducing a vector database (Pinecone) for semantic search and scalable context retrieval. Instead of injecting the entire knowledge base into the prompt, we now embed each chunk using OpenAI's embedding model and store it in Pinecone for efficient similarity-based querying.\n",
        "\n",
        "### 🔍 What’s New:\n",
        "- Embeds knowledge base chunks using `text-embedding-ada-002`\n",
        "- Stores vectors in Pinecone with namespace partitioning\n",
        "- Retrieves top-k relevant chunks based on user query embeddings\n",
        "- Injects only the most relevant context into GPT-4 prompts\n",
        "- Enables scalable, multi-tenant, and agentic workflows\n",
        "\n",
        "### 🧰 Benefits Over Static RAG:\n",
        "- Faster and more focused responses\n",
        "- Reduced token usage and prompt clutter\n",
        "- Modular architecture for dynamic knowledge injection\n",
        "- Supports real-time updates and multi-domain retrieval\n",
        "\n",
        "> This upgrade transforms IonnovaBot into a context-aware assistant capable of reasoning over large, evolving knowledge bases with precision and speed.\n"
      ],
      "metadata": {
        "id": "JDY-KVyONcFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"GPT_API_KEY\")\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index_name = \"ionnova-rag\"\n",
        "if index_name not in pc.list_indexes():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Load and embed knowledge base\n",
        "def load_and_embed_kb():\n",
        "    with open(\"knowledge_base.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        chunks = f.read().split(\"\\n\\n\")\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        embedding = client.embeddings.create(\n",
        "            model=\"text-embedding-ada-002\",\n",
        "            input=chunk\n",
        "        ).data[0].embedding\n",
        "        index.upsert([(f\"chunk-{i}\", embedding, {\"text\": chunk})],namespace=\"ionnova\")\n",
        "\n",
        "# Retrieve top-k relevant chunks\n",
        "def retrieve_context(query, top_k=5):\n",
        "    query_embedding = client.embeddings.create(\n",
        "        model=\"text-embedding-ada-002\",\n",
        "        input=query\n",
        "    ).data[0].embedding\n",
        "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True, namespace=\"ionnova\")\n",
        "    return [match[\"metadata\"][\"text\"] for match in results[\"matches\"]]\n",
        "\n",
        "# Chat loop\n",
        "def chat_with_rag():\n",
        "    print(\"Welcome to IonnovaBot! Type 'exit' to quit.\\n\")\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant that uses external knowledge to answer questions.\"}]\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"IonnovaBot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        context_chunks = retrieve_context(user_input)\n",
        "        context = \"\\n---\\n\".join(context_chunks)\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {user_input}\"})\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        reply = response.choices[0].message.content\n",
        "        print(f\"IonnovaBot: {reply}\")\n",
        "        messages.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "# Run once to populate Pinecone\n",
        "load_and_embed_kb()\n",
        "\n",
        "chat_with_rag()\n"
      ],
      "metadata": {
        "id": "8JCNsWdJFuvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_MODEL = \"text-embedding-3-small\"  # 1536-dim\n",
        "EMBED_DIM = 1536\n",
        "\n",
        "def embed(text: str):\n",
        "    \"\"\"Return a single 1536-dim embedding vector for the given text.\"\"\"\n",
        "    resp = client.embeddings.create(model=EMBED_MODEL, input=[text])\n",
        "    return resp.data[0].embedding\n",
        "\n",
        "\n",
        "query_text = \"Who are Ionnova.\"\n",
        "query_vec = embed(query_text)\n",
        "\n",
        "def run_query(index, vector, top_k=5, with_metadata=True):\n",
        "    return index.query(\n",
        "        vector=vector,\n",
        "        top_k=top_k,\n",
        "        include_metadata=with_metadata\n",
        "    )\n",
        "\n",
        "res_cosine = run_query(index, query_vec)\n",
        "print(res_cosine)"
      ],
      "metadata": {
        "id": "1r-GiNo_JuA0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}